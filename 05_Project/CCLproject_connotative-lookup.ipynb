{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5a82bc5",
   "metadata": {},
   "source": [
    "aniiyer@iu.edu\n",
    "CCL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59f3733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import mkdir, getcwd, chdir, listdir\n",
    "from os.path import join\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35f1ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !rm -rf bert\n",
    "# # !git clone https://github.com/google-research/bert\n",
    "# import sys\n",
    "# sys.path.append('bert/')\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import codecs\n",
    "# import collections\n",
    "# import json\n",
    "# import re\n",
    "# import os\n",
    "# import pprint\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import modeling\n",
    "# import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fe9545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting translate-api\n",
      "  Downloading translate_api-4.9.5-py3-none-any.whl (20 kB)\n",
      "Collecting PyExecJS>=1.5.1\n",
      "  Downloading PyExecJS-1.5.1.tar.gz (13 kB)\n",
      "Collecting loguru>=0.4.1\n",
      "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
      "Collecting pathos>=0.2.7\n",
      "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: lxml>=4.5.0 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from translate-api) (4.6.3)\n",
      "Requirement already satisfied: requests>=2.25.1 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from translate-api) (2.25.1)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from loguru>=0.4.1->translate-api) (0.4.4)\n",
      "Collecting win32-setctime>=1.0.0\n",
      "  Downloading win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
      "Collecting multiprocess>=0.70.14\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "Collecting ppft>=1.7.6.6\n",
      "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
      "Collecting pox>=0.3.2\n",
      "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
      "Collecting dill>=0.3.6\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from PyExecJS>=1.5.1->translate-api) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anirudh\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (2020.12.5)\n",
      "Building wheels for collected packages: PyExecJS\n",
      "  Building wheel for PyExecJS (setup.py): started\n",
      "  Building wheel for PyExecJS (setup.py): finished with status 'done'\n",
      "  Created wheel for PyExecJS: filename=PyExecJS-1.5.1-py3-none-any.whl size=14588 sha256=5c71625de677ec29cfed02a29191036b1be645a3bdc77b1cb02e5454e9287a73\n",
      "  Stored in directory: c:\\users\\anirudh\\appdata\\local\\pip\\cache\\wheels\\db\\3c\\3d\\7e9aca234caf6602ae4a4c7b367b3afc03519e791b998a94e4\n",
      "Successfully built PyExecJS\n",
      "Installing collected packages: dill, win32-setctime, ppft, pox, multiprocess, PyExecJS, pathos, loguru, translate-api\n",
      "Successfully installed PyExecJS-1.5.1 dill-0.3.6 loguru-0.6.0 multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 translate-api-4.9.5 win32-setctime-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/translate-api/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/translate-api/\n"
     ]
    }
   ],
   "source": [
    "pip install translate-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83d48ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dictionary = {}\n",
    "DIRECTORY = getcwd()\n",
    "with open(join(DIRECTORY, 'GloVe', 'glove.6B.300d.txt'),encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word, glove_vector = line.split(' ', maxsplit=1)\n",
    "        glove_vector = np.array(glove_vector.split(' ')).astype('float')\n",
    "        embedded_dictionary.update({word:glove_vector})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bce0e7",
   "metadata": {},
   "source": [
    "Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83054ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentences, embedded_dictionary, \n",
    "                       pos_tags = ['NN','NNS','NNP','NNPS','JJ','RB','VB',\n",
    "                                      'VBG','VBN','VBP','VBZ','VBD']):\n",
    "\n",
    "    sentences = sentences.replace('\\n', ' ').replace('_', \"\").lower()\n",
    "    tokens = nltk.word_tokenize(sentences)\n",
    "\n",
    "    # tokens decide if a particular pos_tags is in the sentence or not.\n",
    "    PoS = np.array([int(tag[1] in pos_tags) \n",
    "                         for tag in nltk.pos_tag(tokens)]).reshape((-1,1))\n",
    "\n",
    "    # apply pos to the embeddings\n",
    "    dictionary_dimensions = len(list(embedded_dictionary.values())[0])\n",
    "    embedded_sentence = np.array([embedded_dictionary[token] \n",
    "                          if token in embedded_dictionary.keys() \n",
    "                          else np.zeros(dictionary_dimensions) \n",
    "                          for token in tokens])\n",
    "    embedded_sentence *= PoS\n",
    "\n",
    "    return embedded_sentence\n",
    "\n",
    "def cos_dist(vec1, vec2):\n",
    "\n",
    "    if np.sqrt(np.sum(vec1 ** 2)) == 0 or np.sqrt(np.sum(vec2 ** 2)) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 - np.dot(vec1, vec2) / (np.sqrt(np.sum(vec1 ** 2)) * np.sqrt(np.sum(vec2 ** 2)))\n",
    "\n",
    "def moving_window(search_content, search_query, increase_padding_by = 0):\n",
    "\n",
    "    query_length = search_query.shape[0]\n",
    "    padded_query_length = query_length + increase_padding_by\n",
    "\n",
    "    # bag-of-words excerpt embedding\n",
    "    excerpt_embedding = search_query.mean(axis=0)\n",
    "\n",
    "    # window moving size\n",
    "    current_window = np.array([search_content[line-padded_query_length:line].mean(axis=0) \n",
    "                            for line in range(padded_query_length, search_content.shape[0])])\n",
    "\n",
    "    # sliding distance: distance between sliding window and excerpt embeddings\n",
    "    window_query_cosdistance = np.array([cos_dist(current_window[line,:], \n",
    "                                         excerpt_embedding) \n",
    "                         for line in range(current_window.shape[0])])\n",
    "\n",
    "    return window_query_cosdistance\n",
    "\n",
    "def find_sentence(search_content, pos, len):\n",
    "\n",
    "    search_content = search_content.replace('\\n', ' ').replace('_', \"\").lower()\n",
    "    closest_content = nltk.word_tokenize(search_content)\n",
    "    closest_content = closest_content[pos : pos + len]\n",
    "    closest_content = ' '.join(closest_content).replace(' ,',',').replace(' .','.')\n",
    "#     print(closest_content)\n",
    "    return closest_content\n",
    "\n",
    "def connotative_search(excerpt, book, padding = 4):\n",
    "    # embed search query\n",
    "    book_embedding = embed_sentence(book, embedded_dictionary)\n",
    "    excerpt_embedding = embed_sentence(excerpt, embedded_dictionary)\n",
    "    excerpt_word_count = len(nltk.word_tokenize(excerpt))\n",
    "\n",
    "    # calculate cosine distance/similarity\n",
    "    distances = moving_window(book_embedding, \n",
    "                                 excerpt_embedding, \n",
    "                                 increase_padding_by = padding)\n",
    "\n",
    "    # find minimum cosine distance and locate the respective window containing that sentence\n",
    "    match = find_sentence(book, distances.argmin(), \n",
    "                       excerpt_word_count + padding)\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d06daf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anirudh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3afcd",
   "metadata": {},
   "source": [
    "Here sample_text is the content you want the search window to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ad9f8",
   "metadata": {},
   "source": [
    "Search query and the search content is provided to the locate method\n",
    "The automatic padding provided is by default 5 since it gave me the best results for the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6106f66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest result:- \\n ', `` but, wonderful to relate, not an irregular, shapeless fragment of crude ore, fresh from nature 's crucible\n"
     ]
    }
   ],
   "source": [
    "query = (\"But, wonderfully to tell, not an irregular, shapeless fragment of raw ore fresh from nature's crucible.\")\n",
    "# query = input(\"Enter your search query :-\")\n",
    "# query = query.replace('\\n', ' ').replace('_', \"\").lower()\n",
    "# Document = (input(\"Enter your search Document -\"))\n",
    "Document= (\"sample_text.txt\")\n",
    "with open(Document,encoding='utf-8') as f:\n",
    "    doc = str(f.readlines())\n",
    "\n",
    "book_embedding = sequence_embedding(doc, embedded_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a3b007e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest result:- \\n ', `` but, wonderful to relate, not an irregular, shapeless fragment of crude ore, fresh from nature 's crucible\n"
     ]
    }
   ],
   "source": [
    "print(\"Closest result:-\",connotative_search(query, doc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1be708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
